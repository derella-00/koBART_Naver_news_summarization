{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# transfomer를 사용한 Kobart 모델 파인 튜닝"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.  사전 작업"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BartForConditionalGeneration\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:52.924029Z",
     "end_time": "2023-05-08T21:58:52.939689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "#사전 학습된 모델 가져오기\n",
    "#Hugging face에서 자동적으로 가져옴\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:52.931768Z",
     "end_time": "2023-05-08T21:58:55.638432Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#평가 함수 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    모델  평가 지표 계산 함수\n",
    "    rouge score 퍼센트 반환\n",
    "    \"\"\"\n",
    "\n",
    "    metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.638432Z",
     "end_time": "2023-05-08T21:58:55.655207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## 데이터 전처리 함수 한줄 한줄씩 되어 있는 리스트를 통합하는 전처리 함수\n",
    "def extract_body(article) -> str:\n",
    "\n",
    "    art_sentence = []\n",
    "\n",
    "    for contents in article:\n",
    "        if len(contents) >= 2:\n",
    "            for sub_con in contents:\n",
    "                art_sentence.append(sub_con['sentence'])\n",
    "                continue\n",
    "        elif len(contents) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            art_sentence.append(contents[0]['sentence'])\n",
    "\n",
    "    return art_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.649784Z",
     "end_time": "2023-05-08T21:58:55.669949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 문자들 토큰화 함수\n",
    "def preprocess_function(examples):\n",
    "    prefix = ''\n",
    "    max_input_length = 2000\n",
    "    max_target_length = 750\n",
    "    inputs = [prefix + doc for doc in examples[\"body\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # 데이터가 손실될 경우에 앞의 단어가 아니라 뒤의 단어가 삭제되도록 하고싶다면 truncating이라는 인자를 사용합니다. truncating='post'를 사용할 경우 뒤의 단어가 삭제됩니다.\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"abstractive\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.665446Z",
     "end_time": "2023-05-08T21:58:55.681446Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 문장의 태그 같은 불필요 요소 제거 함수\n",
    "def sentence_validation(art_sentence):\n",
    "    del_sentence = []\n",
    "    for sentence in art_sentence:\n",
    "        if '@' in sentence or '/사진' in sentence:\n",
    "            del_sentence.append(sentence)\n",
    "        elif sentence[-1] != '.':\n",
    "            del_sentence.append(sentence)\n",
    "    for del_sen in del_sentence:\n",
    "        art_sentence.remove(del_sen)\n",
    "    return ' '.join(art_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.682446Z",
     "end_time": "2023-05-08T21:58:55.700670Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 쿠다 환경 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.697158Z",
     "end_time": "2023-05-08T21:58:55.779766Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 데이터 읽기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "file_location = '../data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.774778Z",
     "end_time": "2023-05-08T21:58:55.816892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:58:55.793895Z",
     "end_time": "2023-05-08T21:59:14.242840Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{file_location}train_original.json', encoding='utf-8') as train_f:\n",
    "    train_data = json.loads(train_f.read())\n",
    "    train_df = pd.DataFrame(train_data['documents'])\n",
    "\n",
    "with open(f'{file_location}valid_original.json', encoding='utf-8') as valid_f:\n",
    "    valid_data = json.loads(valid_f.read())\n",
    "    valid_df = pd.DataFrame(valid_data['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 데이터 전처리"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_df['body'] = train_df.text.apply(lambda x: sentence_validation(extract_body(x)))\n",
    "valid_df['body'] = valid_df.text.apply(lambda x: sentence_validation(extract_body(x)))\n",
    "\n",
    "train_df.abstractive = train_df.abstractive.apply(lambda x: x[0])\n",
    "valid_df.abstractive = valid_df.abstractive.apply(lambda x: x[0])\n",
    "\n",
    "train_df.drop(train_df.body.str.len().sort_values(ascending=False).head(1).index[0], inplace=True)\n",
    "\n",
    "dataset = ds.dataset(pa.Table.from_pandas(train_df).to_batches())\n",
    "\n",
    "### convert to Huggingface dataset\n",
    "hg_dataset = Dataset(pa.Table.from_pandas(train_df))\n",
    "# 테스트용으로 validation 데이터 1000개만 사용\n",
    "hg_dataset_test = Dataset(pa.Table.from_pandas(valid_df[:1000]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:59:14.371044Z",
     "end_time": "2023-05-08T21:59:39.660874Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 데이터 토큰화"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/243982 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd2a34b5c51f4dd4a0c387604af89f1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloe\\miniconda3\\envs\\model\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b87e378902fa4f088addece8b5c77874"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = hg_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_data_test = hg_dataset_test.map(preprocess_function, batched=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T21:59:39.697581Z",
     "end_time": "2023-05-08T22:01:23.611899Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. 하이퍼 파라미터 튜닝"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "batch_size = 2\n",
    "model_name = 'kobart_v2'\n",
    "args = Seq2SeqTrainingArguments(\n",
    "\n",
    "    # 이 이름의 폴더를 생성하고 저장됨\n",
    "    f\"{model_name}-full\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # CUDA 설정을 완료한 후 사용가능.\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "print('start training')\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data,\n",
    "    eval_dataset=tokenized_data_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T22:01:23.625135Z",
     "end_time": "2023-05-08T22:01:27.255563Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.트레이닝 및 모델 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloe\\miniconda3\\envs\\model\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='609955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [     2/609955 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000023C545B31F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Chloe\\miniconda3\\envs\\model\\lib\\site-packages\\tqdm\\std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
